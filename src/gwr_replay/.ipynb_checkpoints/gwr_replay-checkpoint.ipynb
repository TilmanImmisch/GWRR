{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "699c2e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch._C import device\n",
    "#from torch._C import long\n",
    "\n",
    "#own imports\n",
    "from eggwr_plus import EGGWR_Plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1871bd26-7324-4d9d-8257-ca1fed08b760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20c724a9-729f-47a4-bb6d-bd09196dbdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# added Parse command line arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--a_t', type=float, help='Activation threshold', default=.95)\n",
    "#parser.add_argument('--batch_size', type=int, help='Batch size for training', default=8)\n",
    "parser.add_argument('--batch_size_ft', type=int, help='Batch size for fine tuning of feature extractors. E.g. BERT for NLP', default=32)\n",
    "parser.add_argument('--datasets', '-d', nargs='+', type=str, help='Datasets used for training', default=['IRIS'])\n",
    "parser.add_argument('--sem_a_t', type=float, help='Activation threshold for semantic memory in GDM model', default=.35)\n",
    "parser.add_argument('--del_freq', type=int, help='Deletion frequency in SOINN network', default=10)\n",
    "parser.add_argument('--delta_plus', type=float, help='Positive Label change rate', default=1)\n",
    "parser.add_argument('--delta_minus', type=float, help='Negative Label change rate', default=0.1)\n",
    "parser.add_argument('--dim', type=int, help='Dimension of growing memory and language model output', default=17)\n",
    "parser.add_argument('--eps_b', type=float, help='Learning rate for weight adaption (BMU)', default=.1)\n",
    "parser.add_argument('--eps_n', type=float, help='Learning rate for weight adaption of (sample)', default=.001)\n",
    "parser.add_argument('--h_t', type=float, help='Habituation/Firing threshold', default=.3)\n",
    "parser.add_argument('--gamma', type=float, help='Learning rate for label weight adaptation', default=.5)\n",
    "parser.add_argument('--kappa', type=float, help='Habituation controlling parameter', default=1.05)\n",
    "parser.add_argument('--learner', type=str, help='Learner method', default='SOINNPLUS')\n",
    "parser.add_argument('--load_latest', type=bool, help='Load latest pretrained model from file', default=False)\n",
    "parser.add_argument('--log_freq', type=int, help='Logging frequency of learning metrics', default=1)\n",
    "parser.add_argument('--lr', type=float, help='Learning rate (language model/feature extractor)', default=3e-5)\n",
    "parser.add_argument('--l_t', type=float, help='Label propagation threshold', default=.5)\n",
    "parser.add_argument('--max_age', type=int, help='Maximum age of a node connecting edge', default=5)\n",
    "parser.add_argument('--max_len', type=int, help='Maximum sequence length for the transformer input', default=20)\n",
    "parser.add_argument('--m_t', type=int, help='Misclassification threshold', default=0)\n",
    "parser.add_argument('--reduce', type=int, help='Maximum number of train/test samples per dataset', default=300)\n",
    "parser.add_argument('--reduce_test', type=int, help='Maximum number of test samples per dataset, if -1 reduce is used as maximum number of samples', default=-1)\n",
    "#parser.add_argument('--seed', type=int, help='Random state for reproducible output', default=42)\n",
    "parser.add_argument('--tau_b', type=float, help='Constant habituation controlling rate (BMU)', default=.3)\n",
    "parser.add_argument('--tau_n', type=float, help='Constant habituation controlling rate (sample)', default=.1)\n",
    "parser.add_argument('--beta', type=float, help='Regulate influence of context on merge vector', default=0.5)\n",
    "parser.add_argument('--n_context', type=int, help='Window size / number of context', default=4)\n",
    "parser.add_argument('--class_list', type=list, help='List of classes', default=None)\n",
    "parser.add_argument('--num_labels', type=int, help='Number of label classes', default=1)\n",
    "parser.add_argument('--num_sentences', type=int, help='Number of Sentences to Split TextClassificationDatasets into, if -1 Dataset is not split into Sentences ', default=5)\n",
    "parser.add_argument('--tuning_share', type=float, help='Share of training data used during fine tuning',default=0.1)\n",
    "parser.add_argument('--bert_finetune', type=bool, help='finetune bert model on data ', default=False)\n",
    "#args = parser.parse_args()\n",
    "\n",
    "#original\n",
    "#parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--policy\", default=\"DDPG\")                  # Policy name (TD3, DDPG or OurDDPG)\n",
    "parser.add_argument(\"--env\", default=\"HalfCheetah-v2\")          # OpenAI gym environment name\n",
    "parser.add_argument(\"--seed\", default=0, type=int)              # Sets Gym, PyTorch and Numpy seeds\n",
    "parser.add_argument(\"--start_timesteps\", default=25e3, type=int)# Time steps initial random policy is used\n",
    "parser.add_argument(\"--eval_freq\", default=5e3, type=int)       # How often (time steps) we evaluate\n",
    "parser.add_argument(\"--max_timesteps\", default=1e6, type=int)   # Max time steps to run environment\n",
    "parser.add_argument(\"--expl_noise\", default=0.1)                # Std of Gaussian exploration noise\n",
    "parser.add_argument(\"--batch_size\", default=256, type=int)      # Batch size for both actor and critic\n",
    "parser.add_argument(\"--discount\", default=0.99)                 # Discount factor\n",
    "parser.add_argument(\"--tau\", default=0.005)                     # Target network update rate\n",
    "parser.add_argument(\"--policy_noise\", default=0.2)              # Noise added to target policy during critic update\n",
    "parser.add_argument(\"--noise_clip\", default=0.5)                # Range to clip target policy noise\n",
    "parser.add_argument(\"--policy_freq\", default=2, type=int)       # Frequency of delayed policy updates\n",
    "parser.add_argument(\"--save_model\", action=\"store_true\")        # Save model and optimizer parameters\n",
    "parser.add_argument(\"--load_model\", default=\"\")                 # Model load file name, \"\" doesn't load, \"default\" uses file_name\n",
    "\n",
    "#my own arguments\n",
    "parser.add_argument(\"--replay_memory\", default=\"gwr_replay\")    # Choose the replay memory to use\n",
    "args = parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3785108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GWR_replay(EGGWR_Plus):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        #Reward Matrix for every node j\n",
    "        self.R = torch.zeros(self.size)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def add(self, state, action, next_state, reward, done):\n",
    "        state = torch.from_numpy(state)\n",
    "        next_state = torch.from_numpy(next_state)\n",
    "        EGGWR_Plus.forward(self,0,[[state,1],[next_state,1]], action, reward)\n",
    "\n",
    "\n",
    "    def sample(self):\n",
    "        s_ind = np.random.randint(0, self.size)\n",
    "        s_ind = torch.tensor(s_ind, dtype=torch.int64)\n",
    "        temporal_temp = self.temporal[s_ind].to_dense()\n",
    "        next_s_ind = torch.argmax(temporal_temp[:,0])\n",
    "        #next_s_ind= torch.argmax(self.temporal[s_ind, :,0])\n",
    "        temporal_edge = self.temporal[s_ind,next_s_ind].to_dense()\n",
    "        print(temporal_edge)\n",
    "        return (\n",
    "            self.V[s_ind].float().to(self.device),\n",
    "            self.V[next_s_ind].float().to(self.device),\n",
    "            temporal_edge[1:7].to(self.device),\n",
    "            temporal_edge[7].to(self.device),\n",
    "            #never done state, because for nodes useless: \n",
    "            1\n",
    "        )\n",
    "\n",
    "    def get_bmu(self, state):\n",
    "        state = torch.from_numpy(state)\n",
    "        bmu, _, _ = self.activate_bmu(state)\n",
    "        return self.V[bmu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1431f4ec-7f99-4ec0-b7d3-cd82aa9301b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = GWR_replay(**vars(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "edf48575-37fb-4d2d-b254-780fa36fd055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = np.random.rand(17)\n",
    "next_state = np.random.rand(17)\n",
    "action = np.array([0,1,2,3,4,5])\n",
    "test.add(state,action,next_state, 5, 0 )\n",
    "\n",
    "test.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ff4fbd86-0092-4bdb-b36a-13e271fcfbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 0., 1., 2., 3., 4., 5., 5.])\n"
     ]
    }
   ],
   "source": [
    "a,b,c,d,e = test.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48634a7-cb0f-439d-8e7b-99916a2081f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a, b, c, d, e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
